Total sparse weights: 74.539 (12815247/17192650)

Sparse weight by layer
Layer: conv1_1.weight, 0.47511574074074076 (821/1728)
Layer: conv2_1.weight, 0.533203125 (39312/73728)
Layer: conv3_1.weight, 0.6095547146267362 (179765/294912)
Layer: fc1.weight, 0.7493541836738586 (12572077/16777216)
Layer: fc1.bias, 0.676513671875 (2771/4096)
Layer: fc2.weight, 0.500390625 (20496/40960)
Layer: fc2.bias, 0.5 (5/10)

Sparse weight by filter
Layer: conv1_1.weight, [14, 12, 11, 15, 11, 17, 11, 12, 12, 8, 13, 12, 10, 14, 12, 12, 13, 15, 15, 11, 15, 9, 11, 12, 15, 13, 12, 12, 13, 14, 11, 9, 12, 13, 14, 12, 27, 12, 17, 13, 14, 12, 9, 12, 12, 9, 14, 16, 7, 13, 12, 14, 15, 11, 10, 12, 9, 14, 12, 15, 11, 27, 13, 12]
Layer: conv2_1.weight, [245, 207, 337, 340, 286, 263, 348, 221, 302, 289, 266, 576, 268, 343, 256, 268, 237, 212, 422, 404, 397, 310, 211, 576, 295, 358, 363, 364, 322, 272, 303, 322, 316, 576, 335, 266, 264, 373, 268, 234, 405, 386, 336, 264, 270, 261, 291, 312, 257, 293, 294, 270, 328, 355, 272, 269, 322, 234, 274, 245, 334, 230, 297, 257, 256, 278, 278, 357, 302, 396, 243, 281, 324, 244, 292, 304, 303, 272, 375, 293, 271, 276, 282, 313, 316, 410, 576, 576, 274, 249, 233, 227, 233, 310, 299, 257, 234, 375, 363, 284, 369, 297, 272, 244, 350, 295, 329, 312, 382, 252, 186, 307, 290, 398, 305, 243, 317, 402, 271, 256, 265, 380, 308, 250, 228, 251, 239, 287]
Layer: conv3_1.weight, [614, 692, 613, 576, 616, 628, 667, 634, 698, 582, 622, 623, 602, 610, 613, 626, 610, 567, 639, 616, 654, 637, 608, 587, 1135, 1152, 621, 612, 661, 611, 1152, 627, 625, 685, 645, 620, 620, 576, 628, 617, 675, 608, 561, 645, 570, 593, 1152, 565, 618, 630, 587, 1152, 1089, 627, 628, 658, 685, 1131, 624, 676, 606, 1152, 597, 1152, 622, 1089, 1129, 644, 619, 632, 586, 623, 606, 708, 576, 616, 565, 646, 642, 646, 616, 604, 1152, 1152, 630, 603, 1138, 616, 685, 623, 593, 584, 635, 594, 645, 646, 637, 618, 658, 693, 619, 583, 703, 609, 607, 1097, 593, 647, 598, 609, 643, 665, 1152, 1152, 635, 619, 656, 628, 586, 610, 578, 631, 620, 638, 658, 610, 578, 676, 674, 1152, 695, 629, 610, 638, 594, 652, 1152, 668, 595, 577, 628, 632, 594, 629, 625, 1152, 585, 603, 730, 599, 609, 581, 1152, 610, 601, 644, 1152, 584, 1152, 587, 599, 1152, 592, 638, 606, 631, 586, 673, 620, 1152, 1152, 625, 644, 1152, 631, 615, 607, 631, 566, 599, 653, 577, 591, 633, 670, 638, 624, 573, 641, 608, 612, 616, 646, 593, 647, 624, 1123, 598, 603, 615, 616, 1152, 613, 1148, 1134, 596, 621, 597, 634, 594, 680, 727, 626, 585, 697, 1152, 553, 592, 613, 613, 1152, 1152, 616, 628, 1152, 611, 601, 612, 1152, 610, 605, 685, 607, 623, 1144, 1152, 572, 632, 600, 632, 653, 588, 1152, 628, 675, 615, 598, 626, 643, 649, 723, 674, 601, 607, 595, 643]
Layer: fc1.weight, None
Layer: fc1.bias, None
Layer: fc2.weight, None
Layer: fc2.bias, None
========== epoch 0
Train: Loss: 13.426, Acc: 68.042 (34021/50000)
Train Time: 54.933
Test: Loss: 1.817, Acc: 67.726 (33863/50000)
Test Time: 7.704
========== epoch 1
Train: Loss: 10.337, Acc: 67.186 (33593/50000)
Train Time: 54.845
Test: Loss: 1.845, Acc: 66.478 (33239/50000)
Test Time: 7.643
========== epoch 2
Train: Loss: 8.613, Acc: 65.900 (32950/50000)
Train Time: 54.900
Test: Loss: 1.869, Acc: 65.572 (32786/50000)
Test Time: 7.637
========== epoch 3
Train: Loss: 7.445, Acc: 64.378 (32189/50000)
Train Time: 54.930
Test: Loss: 1.896, Acc: 63.638 (31819/50000)
Test Time: 7.643
========== epoch 4
Train: Loss: 6.595, Acc: 62.846 (31423/50000)
Train Time: 54.866
Test: Loss: 1.920, Acc: 62.450 (31225/50000)
Test Time: 7.708
========== epoch 5
Train: Loss: 5.949, Acc: 61.092 (30546/50000)
Train Time: 54.819
Test: Loss: 1.943, Acc: 60.414 (30207/50000)
Test Time: 7.637
========== epoch 6
Train: Loss: 5.443, Acc: 59.378 (29689/50000)
Train Time: 54.887
Test: Loss: 1.966, Acc: 57.494 (28747/50000)
Test Time: 7.698
========== epoch 7
Train: Loss: 5.038, Acc: 57.478 (28739/50000)
Train Time: 54.880
Test: Loss: 1.982, Acc: 56.104 (28052/50000)
Test Time: 7.640
========== epoch 8
Train: Loss: 4.709, Acc: 55.526 (27763/50000)
Train Time: 54.899
Test: Loss: 2.000, Acc: 54.196 (27098/50000)
Test Time: 7.642
========== epoch 9
Train: Loss: 4.437, Acc: 53.554 (26777/50000)
Train Time: 54.942
Test: Loss: 2.015, Acc: 52.772 (26386/50000)
Test Time: 7.694
Best Training Accuracy: 68.042%
Best Test Accuracy: 67.726%

Total sparse weights: 68.711 (11813248/17192650)

Sparse weight by layer
Layer: conv1_1.weight, 0.4704861111111111 (813/1728)
Layer: conv2_1.weight, 0.4902886284722222 (36148/73728)
Layer: conv3_1.weight, 0.5829298231336806 (171913/294912)
Layer: fc1.weight, 0.6904146075248718 (11583235/16777216)
Layer: fc1.bias, 0.65185546875 (2670/4096)
Layer: fc2.weight, 0.45078125 (18464/40960)
Layer: fc2.bias, 0.5 (5/10)

Sparse weight by filter
Layer: conv1_1.weight, [14, 13, 10, 15, 10, 17, 10, 12, 12, 8, 13, 12, 9, 14, 12, 11, 13, 15, 16, 11, 15, 9, 11, 12, 15, 13, 12, 12, 13, 14, 11, 8, 12, 13, 14, 12, 27, 12, 17, 13, 14, 12, 9, 12, 10, 8, 14, 16, 7, 13, 12, 14, 15, 12, 9, 12, 8, 14, 12, 15, 11, 27, 13, 12]
Layer: conv2_1.weight, [202, 173, 375, 381, 281, 266, 389, 146, 324, 307, 230, 576, 260, 322, 204, 239, 179, 134, 260, 338, 230, 243, 150, 576, 268, 366, 395, 334, 294, 282, 313, 300, 344, 576, 279, 246, 257, 435, 262, 167, 191, 408, 358, 244, 219, 240, 286, 281, 239, 345, 292, 243, 360, 344, 259, 202, 293, 180, 248, 196, 304, 195, 296, 238, 213, 251, 264, 328, 329, 234, 188, 279, 364, 188, 288, 275, 310, 225, 393, 238, 264, 260, 239, 297, 278, 385, 576, 576, 228, 190, 186, 142, 182, 328, 275, 199, 209, 388, 403, 293, 386, 227, 216, 234, 373, 315, 355, 371, 375, 199, 119, 306, 285, 389, 247, 208, 334, 447, 252, 193, 204, 197, 251, 220, 143, 204, 218, 251]
Layer: conv3_1.weight, [513, 572, 649, 557, 512, 630, 727, 685, 634, 491, 580, 618, 635, 548, 526, 675, 579, 457, 504, 587, 520, 592, 591, 525, 1152, 205, 594, 557, 650, 530, 1152, 655, 544, 568, 501, 578, 634, 382, 482, 348, 619, 667, 456, 506, 463, 523, 280, 538, 540, 507, 585, 1150, 1068, 629, 614, 840, 1152, 1152, 637, 642, 597, 1152, 598, 1152, 433, 1152, 1152, 648, 575, 652, 431, 516, 444, 764, 505, 604, 562, 1152, 604, 586, 611, 556, 1152, 1152, 613, 530, 1106, 593, 655, 657, 644, 516, 652, 575, 601, 699, 748, 564, 628, 737, 480, 478, 1152, 476, 552, 1152, 602, 567, 503, 690, 640, 526, 1152, 1152, 705, 719, 473, 684, 460, 463, 429, 1152, 536, 677, 664, 489, 524, 602, 630, 1152, 598, 657, 526, 627, 585, 779, 1152, 423, 525, 460, 551, 431, 590, 553, 531, 1152, 567, 551, 557, 594, 659, 597, 1152, 541, 674, 539, 1152, 592, 1152, 549, 546, 1152, 604, 679, 587, 709, 575, 723, 619, 1152, 1107, 490, 752, 718, 636, 724, 487, 635, 540, 581, 553, 527, 578, 602, 849, 500, 618, 561, 629, 681, 688, 621, 708, 571, 627, 699, 1152, 502, 639, 497, 624, 1152, 583, 1152, 1152, 622, 661, 507, 592, 616, 675, 1047, 533, 507, 832, 1152, 512, 584, 586, 598, 1152, 1152, 529, 584, 1152, 667, 568, 627, 1152, 576, 567, 739, 552, 635, 1152, 76, 441, 525, 616, 546, 756, 473, 1152, 661, 555, 667, 629, 446, 576, 1152, 660, 653, 607, 466, 565, 610]
Layer: fc1.weight, None
Layer: fc1.bias, None
Layer: fc2.weight, None
Layer: fc2.bias, None
Test: Loss: 2.015, Acc: 52.548 (26274/50000)
Final test accuracy: 52.547999999999995